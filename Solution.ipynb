{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pento Machine Learning Engineer Challenge</h1>\n",
    "<h3>Juan Andres Tabarez Santias's solution</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Summary</h4>\n",
    "\n",
    "As the dataset is very short, I think the best solution is using a pre-trained model.\n",
    "The chosen one is ResNet18, and the way I'm going to use it is by replacing the output layer of the model with my own output layer, this allows me to use the output of the pre-trained model as input of my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I'll start by importing the libraries I'm gonna use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I load the model, setting the parameter pretrained to True, this means the model has been trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juan/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/juan/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/juan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 48.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use compose to these transforms. First I resize the image to 224x224, because it is the input expected by the model ResNet18. Second I convert it to a tensor, because I'm working with PyTorch and this framework require the input data to be in the form of a tensor. And finally I normalize the image using mean and standard desviation for each channel, the values are taken from the dataset the model was pre-trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the respective folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root='./pento-ssr-challenge/dogs', transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the dataset, 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I will only change the output layer of the pre-trained model, I'm going to set the parameters requires grad to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll replace the output layer with my own fully connected layer, as the last hidden layer of the pre-trained model has 512 neurons it will have 512 inputs, and 4 outputs, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataLoader object for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the criterion function and the optimizer object. The optimizer only use parameters where the grad attrubute is set to True, so I only update the parameters of the last layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I train the model using Mini-Batch Stochastic Gradient Descent and print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model(x)\n",
    "        loss = criterion(z, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0\n",
    "    for x_test, y_test in val_loader:\n",
    "        model.eval()\n",
    "        z = model(x_test)\n",
    "        _,yhat = torch.max(z.data, 1)\n",
    "        correct += (yhat == y_test).sum().item()\n",
    "\n",
    "    accuracy = correct / len(val_dataset)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class and the code below allow us to make a prediction of an image using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Image\n",
    "\n",
    "def image_prediction(image_path):\n",
    "    image = composed(Image.open(image_path))\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    _, prediction = torch.max(output, 1)\n",
    "    return dataset.classes[prediction.item()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= 'path to image'\n",
    "predicted_class = image_prediction(image_path)\n",
    "print(f\"The predicted class is:  {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
